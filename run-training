gcloud alpha compute tpus tpu-vm ssh tpu-v4-8 --zone us-central2-b --worker=all --command=' \
python3 $HOME/transformers/examples/flax/language-modeling/run_t5_mlm_flax.py \
--output_dir="./en-t5-base" \
--model_type="t5" \
--config_name="google/flan-t5-small" \
--tokenizer_name="google/flan-t5-small" \
--dataset_name="datablations/c4-filter-small" \
--max_seq_length="512" \
--per_device_train_batch_size="32" \
--per_device_eval_batch_size="32" \
--adafactor \
--learning_rate="0.005" \
--weight_decay="0.001" \
--warmup_steps="2000" \
--overwrite_output_dir \
--logging_steps="500" \
--save_steps="10000" \
--eval_steps="2500" \
--push_to_hub \
'
